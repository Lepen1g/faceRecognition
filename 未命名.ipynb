{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.      0.06395 0.178  ]\n",
      " [0.02326 0.04651 0.178  ]\n",
      " [0.      0.02907 0.297  ]\n",
      " ...\n",
      " [0.16279 0.06395 0.379  ]\n",
      " [0.13889 0.09722 0.441  ]\n",
      " [0.05556 0.0625  0.354  ]]\n",
      "****************nuber****************:  591\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Cost after iteration 0: 6.381807\n",
      "Cost after iteration 1000: 6.376954\n",
      "Cost after iteration 2000: 6.287910\n",
      "Cost after iteration 3000: 6.225591\n",
      "Cost after iteration 4000: 6.190528\n",
      "Cost after iteration 5000: 6.170217\n",
      "Cost after iteration 6000: 6.158694\n",
      "Cost after iteration 7000: 6.151520\n",
      "Cost after iteration 8000: 6.146560\n",
      "Cost after iteration 9000: 6.142769\n",
      "Cost after iteration 10000: 6.139531\n",
      "Cost after iteration 11000: 6.136364\n",
      "Cost after iteration 12000: 6.132753\n",
      "Cost after iteration 13000: 6.128043\n",
      "Cost after iteration 14000: 6.121365\n",
      "Cost after iteration 15000: 6.111781\n",
      "Cost after iteration 16000: 6.098932\n",
      "Cost after iteration 17000: 6.083787\n",
      "Cost after iteration 18000: 6.068291\n",
      "Cost after iteration 19000: 6.054071\n",
      "Cost after iteration 20000: 6.041795\n",
      "Cost after iteration 21000: 6.031458\n",
      "Cost after iteration 22000: 6.022805\n",
      "Cost after iteration 23000: 6.015555\n",
      "Cost after iteration 24000: 6.009460\n",
      "Cost after iteration 25000: 6.004314\n",
      "Cost after iteration 26000: 5.999944\n",
      "Cost after iteration 27000: 5.996207\n",
      "Cost after iteration 28000: 5.992985\n",
      "Cost after iteration 29000: 5.990181\n",
      "(69, 1)\n",
      "[[2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[ 0.13953  0.07558  0.196  ]\n",
      " [ 0.13889  0.0625   0.283  ]\n",
      " [ 0.08333  0.08333  0.535  ]\n",
      " [ 0.13953  0.05814  0.431  ]\n",
      " [ 0.16279  0.05814  0.233  ]\n",
      " [ 0.11628  0.06977  0.435  ]\n",
      " [ 0.09302  0.05814  0.379  ]\n",
      " [ 0.13889  0.08333  0.542  ]\n",
      " [ 0.13953  0.05814  0.297  ]\n",
      " [ 0.       0.06977  0.347  ]\n",
      " [ 0.06977  0.06977  0.19   ]\n",
      " [ 0.09302  0.06977  0.522  ]\n",
      " [ 0.16667  0.0625   0.21   ]\n",
      " [ 0.06977  0.06977  0.394  ]\n",
      " [ 0.11628  0.06395  0.733  ]\n",
      " [ 0.09302  0.06977  0.283  ]\n",
      " [ 0.11628  0.0814   0.254  ]\n",
      " [ 0.16667  0.09722  0.5    ]\n",
      " [ 0.11111  0.06944  0.2    ]\n",
      " [ 0.06977  0.06977  0.254  ]\n",
      " [ 0.11628  0.06977  0.26   ]\n",
      " [ 0.09302  0.06395  0.297  ]\n",
      " [ 0.04651  0.06977  0.34   ]\n",
      " [ 0.06977  0.03488  0.395  ]\n",
      " [ 0.09302  0.03488  0.229  ]\n",
      " [ 0.04651  0.0407   0.262  ]\n",
      " [ 0.06977  0.0407   0.233  ]\n",
      " [ 0.04651  0.0407   0.174  ]\n",
      " [ 0.06977  0.03488  0.3    ]\n",
      " [ 0.06977  0.03488  0.304  ]\n",
      " [ 0.06977  0.04651  0.196  ]\n",
      " [ 0.04651  0.02326  0.308  ]\n",
      " [ 0.09302  0.03488  0.395  ]\n",
      " [ 0.06977  0.0407   0.196  ]\n",
      " [ 0.06977  0.04651  0.078  ]\n",
      " [ 0.02326  0.02326  0.395  ]\n",
      " [ 0.02326  0.02326  0.2    ]\n",
      " [ 0.06977  0.02907  0.379  ]\n",
      " [ 0.02326  0.02907  0.391  ]\n",
      " [ 0.06977  0.03488  0.207  ]\n",
      " [ 0.09302  0.02907  0.347  ]\n",
      " [ 0.06977  0.04651  0.178  ]\n",
      " [ 0.06977  0.02326  0.26   ]\n",
      " [ 0.09302  0.03488  0.292  ]\n",
      " [ 0.06977  0.02326  0.178  ]\n",
      " [ 0.06977  0.04651  0.528  ]\n",
      " [ 0.02326  0.0407   0.069  ]\n",
      " [-0.04651  0.05814  0.196  ]\n",
      " [ 0.02326  0.0407   0.196  ]\n",
      " [-0.02326  0.04651  0.175  ]\n",
      " [-0.02326  0.04651  0.262  ]\n",
      " [ 0.       0.04651  0.069  ]\n",
      " [ 0.04651  0.06977  0.21   ]\n",
      " [ 0.       0.0407   0.174  ]\n",
      " [ 0.       0.07558  0.204  ]\n",
      " [ 0.       0.05233  0.293  ]\n",
      " [ 0.02326  0.04651  0.21   ]\n",
      " [ 0.       0.05814  0.262  ]\n",
      " [ 0.       0.02326  0.466  ]\n",
      " [ 0.02326  0.04651  0.153  ]\n",
      " [ 0.02326  0.04651  0.178  ]\n",
      " [ 0.       0.04651  0.145  ]\n",
      " [ 0.       0.02326  0.123  ]\n",
      " [-0.02326  0.06977  0.056  ]\n",
      " [ 0.02326  0.04651  0.153  ]\n",
      " [ 0.       0.05233  0.21   ]\n",
      " [ 0.02326  0.05814  0.347  ]\n",
      " [ 0.       0.03488  0.164  ]\n",
      " [ 0.02326  0.04651  0.347  ]]\n",
      "[[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "angry\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "surpise\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "happy\n",
      "surpise\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "surpise\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "happy\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "angry\n",
      "surpise\n",
      "angry\n",
      "happy\n",
      "0.9130434782608695\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "#参数初始化，n_x训练样本数量，n_h神经元，n_y分类的数量\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01 #32 * 3\n",
    "    b1 = np.zeros((n_h,1)) #32 * 1\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01 #3 * 32\n",
    "    b2 = np.zeros((n_y,1))#3 * 1\n",
    "  \n",
    "    assert (W1.shape == (n_h, n_x))#32 * 500\n",
    "    assert (b1.shape == (n_h, 1)) #32 * 1\n",
    "    assert (W2.shape == (n_y, n_h))#3 * 32\n",
    "    assert (b2.shape == (n_y, 1))#3 * 1\n",
    "    \n",
    "    parameters = {\"W1\": W1,\"b1\": b1,\"W2\": W2,\"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    " \n",
    "def forward_propagation(X, parameters):\n",
    "   \n",
    "    #参数传递\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    " \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1,X)+b1 #32 * 3 dot 3 * 500\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2 #3 *32  dot 32 * 3\n",
    "    A2 = sigmoid(Z2)\n",
    "    #print(\"A2\",A2)\n",
    "    #print(A2.shape)\n",
    "    \n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n",
    " \n",
    "def compute_cost(A2, Y, parameters):\n",
    "    #计算损失时，用的是转换之后的标签\n",
    "    #m = Y.shape[1] # number of example\n",
    " \n",
    "    # Compute the cross-entropy cost\n",
    "    #softmax\n",
    "    logprobs = -np.multiply(np.log(A2),Y)#交叉商验证，Y\n",
    "    cost = np.sum(logprobs)/A2.shape[1]\n",
    "    #print(logprobs.shape)\n",
    "    #print(cost.shape)\n",
    "    return cost\n",
    " \n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    #反向传播时，用的也是转换之后的标签Y->label_train\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2-Y\n",
    "    dW2 = np.dot(dZ2,A1.T)/m\n",
    "    db2 = np.sum(dZ2,axis=1,keepdims=True)/m\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(1 - np.power(A1, 2))\n",
    "    dW1 =  np.dot(dZ1,X.T)/m\n",
    "    db1 = np.sum(dZ1,axis=1,keepdims=True)/m\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    " \n",
    "# GRADED FUNCTION: update_parameters\n",
    " \n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "   \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    " \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1-learning_rate*dW1\n",
    "    b1 = b1-learning_rate*db1\n",
    "    W2 = W2-learning_rate*dW2\n",
    "    b2 = b2-learning_rate*db2\n",
    "   \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    " \n",
    "#softmax函数定义\n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs\n",
    "def sigmoid(x):\n",
    "    s=1/(1+np.exp(-x))\n",
    "    return s\n",
    "''' \n",
    "data = pd.read_excel('train2018.xlsx',header = None)\n",
    "train = np.array(data)\n",
    " \n",
    "X = train[:,0:100]\n",
    "Y = train[:,100]\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "#print(X.shape)\n",
    "#print(X)\n",
    "#print(Y.shape) # 1*6100\n",
    "#print(Y)\n",
    " \n",
    "X = X.T\n",
    "Y = Y.T\n",
    "n_x = X.shape[0]\n",
    "n_h = 32\n",
    "n_y = 3\n",
    "''' \n",
    "data = []\n",
    "Y = []\n",
    "with open('/home/leipeng/emRedata.txt', 'r') as f:#with语句自动调用close()方法\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        eachline = line.split()###按行读取文本文件，每行数据以列表形式返回\n",
    "        read_data = [ float(x) for x in eachline[0:3] ] #TopN概率字符转换为float型\n",
    "        #print(read_data)\n",
    "        lable = [ int(x) for x in eachline[-1] ]#lable转换为int型\n",
    "        Y.append(lable)\n",
    "        read_data.append(int(lable[0]))\n",
    " \n",
    "        #read_data = list(map(float, eachline))\n",
    "        data.append(read_data)\n",
    "\n",
    "        line = f.readline()\n",
    "Y = np.array(Y)\n",
    "#print(Y.shape)\n",
    "#print(Y)\n",
    "data = np.array(data)\n",
    "X = data[:,:-1]\n",
    "print(X)\n",
    " \n",
    "X = X.T\n",
    "Y = Y.T\n",
    "n_x = X.shape[0]\n",
    "n_h = 32\n",
    "n_y = 3\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)#3 32 3\n",
    " \n",
    " \n",
    "num_iterations = 30000\n",
    "nuber = Y.shape[1] #nuber = 591\n",
    "print(\"****************nuber****************: \",nuber)\n",
    "label_train=np.zeros((3,nuber))# 3 * 591\n",
    "#根据Y值，转换分类标签，这里有三类输出:{0，1，2}\n",
    "#Y为1*591，转换之后为3*591\n",
    "print(label_train)\n",
    "for k in range(nuber):\n",
    "    if Y[0][k] == 0:\n",
    "        label_train[0][k] = 1\n",
    "    if Y[0][k] == 1:\n",
    "        label_train[1][k] = 1\n",
    "    if Y[0][k] == 2:\n",
    "        label_train[2][k] = 1\n",
    "    #label_train[int(Y[0][i])][i]=1\n",
    "    \n",
    "#print(label_train.shape)\n",
    "#print(label_train)\n",
    "\n",
    "#print(label_train.shape)\n",
    "#print(label_train)\n",
    "for i in range(0, num_iterations):\n",
    "    learning_rate=0.05\n",
    "    #if i>1000:\n",
    "       # learning_rate=learning_rate*0.999\n",
    "    # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    #print(A2)   \n",
    "    pre_model = softmax(A2)\n",
    "    #pre_model = A2\n",
    "        \n",
    "    # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "    cost = compute_cost(pre_model, label_train, parameters)\n",
    " \n",
    "    # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "    grads = backward_propagation(parameters, cache, X, label_train)\n",
    " \n",
    "    # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)  \n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "    if i % 1000 == 0:\n",
    "        print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        plt.plot(i, cost, 'ro')\n",
    "\n",
    "#******************************数据测试****************************        \n",
    "data = []\n",
    "Y = []\n",
    "with open('/home/leipeng/testEmRedata.txt', 'r') as f:#with语句自动调用close()方法\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        eachline = line.split()###按行读取文本文件，每行数据以列表形式返回\n",
    "        #print(eachline)\n",
    "        #print(eachline[3])\n",
    "        read_data = [ float(x) for x in eachline[0:3] ] #TopN概率字符转换为float型\n",
    "        #print(read_data)\n",
    "        lable = [ int(x) for x in eachline[3] ]#lable转换为int型\n",
    "        Y.append(lable)\n",
    "        read_data.append(int(lable[0]))\n",
    " \n",
    "        #read_data = list(map(float, eachline))\n",
    "        data.append(read_data)\n",
    "        line = f.readline()\n",
    "Y = np.array(Y)\n",
    "print(Y.shape)\n",
    "print(Y)\n",
    "data = np.array(data)\n",
    "X_test = data[:,:-1]\n",
    "print(X_test)\n",
    "\n",
    "Y_label = Y\n",
    "Y_label = Y_label.reshape((Y_label.shape[0],1))\n",
    " \n",
    "X_test = X_test.T\n",
    "Y_label = Y_label.T\n",
    "print(Y_label)\n",
    "count = 0\n",
    "a2,xxx=forward_propagation(X_test,parameters)\n",
    " \n",
    "for j in range(Y_label.shape[1]):\n",
    "    #np.argmax返回最大值的索引\n",
    "    predict_value=np.argmax(a2[:,j])\n",
    "    #在val上计算准确度\n",
    "    if predict_value==int(Y_label[:,j]):\n",
    "        count=count+1\n",
    "    if predict_value == 0:\n",
    "        print(\"angry\")\n",
    "    elif predict_value == 1:\n",
    "        print(\"happy\")\n",
    "    else:\n",
    "        print(\"surpise\")\n",
    "print(np.divide(count,Y_label.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
